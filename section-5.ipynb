{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15baf9a7",
   "metadata": {},
   "source": [
    "# Important Components of Langchain\n",
    "\n",
    "RAG Application - Given a Datasource (like a PDF), you ask a question and it\n",
    "asks the data set for the answer. \n",
    "\n",
    "- Data Ingestion: Load the data source (pdf, excel, json, video, etc.)\n",
    "- Split/Data Transformation: Divide the data into smaller chucks (text or document chuncks)\n",
    "    - LLM Limitation: Context Size for each model, so you need to break it into pieces\n",
    "- Embedding: Convert everything to vectors so you can use similarity search\n",
    "- Store: Put it in a vectorstor DB (FAISS, chromadb, astradb)\n",
    "\n",
    "The vectordb is the thing that is queried\n",
    "\n",
    "Part 2\n",
    "\n",
    "- Users ask questions\n",
    "- Combine with a prompt template\n",
    "\n",
    "1. Retrieval Chain: interface for querying the vectorstoredb\n",
    "2. Context info is returned\n",
    "3. This is sent to the LLM for an answer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
