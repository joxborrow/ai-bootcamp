{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you set up langchain tracing\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x705612fd7dc0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x705612870b80> root_client=<openai.OpenAI object at 0x705612fd7d30> root_async_client=<openai.AsyncOpenAI object at 0x705612e346a0> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI refers to a subset of artificial intelligence techniques used to create new content, data, or information that is similar to existing examples. It involves algorithms and models, particularly those in machine learning, that can generate text, images, music, and other forms of media.\\n\\nKey approaches in generative AI include:\\n\\n1. **Generative Adversarial Networks (GANs):** Consist of two neural networks, a generator and a discriminator, that are trained together. The generator creates new data instances, while the discriminator evaluates them. The competition between the two helps the generator produce highly realistic outputs.\\n\\n2. **Variational Autoencoders (VAEs):** These models encode input data into a compressed representation, then decode it back into data similar to the input. They can be used to generate new data points by sampling from the latent space.\\n\\n3. **Transformer-based Models:** These include models like GPT (Generative Pre-trained Transformer) that use large datasets to generate human-like text. They leverage the transformer architecture to capture long-range dependencies and complex patterns in data.\\n\\nGenerative AI has numerous applications, such as:\\n\\n- **Text Generation:** Creating articles, poetry, and dialogue that mimic human writing styles.\\n- **Image Generation:** Producing high-quality images based on textual descriptions or style.\\n- **Music Composition:** Generating music that can mimic different genres or styles.\\n- **Simulations and Design:** Creating virtual environments or designing products by exploring a large space of possibilities.\\n\\nOverall, generative AI is transformative across industries, driving advancements in automation, creativity, and innovation.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 318, 'prompt_tokens': 13, 'total_tokens': 331, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-C3W6ckLpiIaezSfrXIASeKdUWZ44Q', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--31690b9e-9a0d-4b8f-92e5-8a66e9f42bb2-0' usage_metadata={'input_tokens': 13, 'output_tokens': 318, 'total_tokens': 331, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "# You can use prompt templates to control the GenAi models\n",
    "# \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith is a developer tool suite designed by the team behind LangChain. It is crafted to facilitate the building, monitoring, and testing of applications that utilize large language models (LLMs). Langsmith offers features that enhance the debugging and optimization processes for LLM-based applications. Key capabilities include in-depth analytics, error tracking, and performance metrics, which allow developers to gain insights into how their applications are interacting with language models. The suite aims to streamline the development process, ensuring that applications built on LLMs are robust, efficient, and scalable.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 33, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff25b2783a', 'id': 'chatcmpl-C3WG1dUmPlyakGibeP15uqVXbiUcG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--c0e4f7c5-d011-4324-84a4-16f9fb4084ad-0' usage_metadata={'input_tokens': 33, 'output_tokens': 110, 'total_tokens': 143, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "## You create a chain with the | character \n",
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a tool designed to enhance the development and deployment of applications using large language models (LLMs) and AI chains. It provides a suite of features that enables developers and engineers to have better control and insight into their AI-driven applications. Key aspects of Langsmith include:\n",
      "\n",
      "1. **Feedback Collection:** Langsmith supports gathering user feedback to improve model performance. This feedback can be structured, allowing developers to iteratively refine their models based on how well they perform in real-world scenarios.\n",
      "\n",
      "2. **Analytics and Monitoring:** It offers analytics tools to monitor the performance of language models and chains. By providing insights into metrics like response times, accuracy, and failure rates, developers can pinpoint issues and optimize their systems.\n",
      "\n",
      "3. **Experimentation Platform:** Langsmith allows developers to run experiments with different models and configurations. This helps in testing variations and selecting the best-performing models for deployment.\n",
      "\n",
      "4. **Seamless Integration:** Designed to be easily integrated into existing workflows and with popular machine learning and AI tools, Langsmith can complement the technologies that developers are already using in their AI pipelines.\n",
      "\n",
      "Overall, Langsmith aims to streamline the process of building, deploying, and maintaining reliable and efficient applications that utilize LLMs by providing vital tools for monitoring, feedback, and optimization.\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser - This can be a part of the chain\n",
    "# The output parser is resoponsible for processing the output\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
